{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.3 64-bit ('venv': venv)",
   "display_name": "Python 3.8.3 64-bit ('venv': venv)",
   "metadata": {
    "interpreter": {
     "hash": "aa7edafa690141b33a974693dc3d5a1bbeaee5f2af1c8eecf097b5936ab79080"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Parte 1: Preliminares: funciones de activación y función de error"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Derivative of the activation functions\n",
    "\n",
    "### *ReLU*\n",
    "\n",
    "$$\n",
    "  \\frac{\\partial}{\\partial x}\\mathit{ReLU}(x) =\n",
    "    \\frac{\\partial}{\\partial x}\\max\\{0, x\\} =\n",
    "    \\begin{cases}\n",
    "      1 & \\text{if } x > 0  \\\\\n",
    "      0 & \\text{otherwise}\n",
    "    \\end{cases}\n",
    "$$\n",
    "\n",
    "### *Swish*\n",
    "\n",
    "Considerando que\n",
    "$$\n",
    "  \\frac{\\partial}{\\partial x}\\sigma(x) = 1 - \\sigma(x)\n",
    "$$\n",
    "\n",
    "Se tiene\n",
    "$$\n",
    "  \\begin{aligned}\n",
    "    \\frac{\\partial}{\\partial x}\\mathit{swish}(x, \\beta)\n",
    "      &= \\frac{\\partial}{\\partial x}(x \\cdot \\sigma(\\beta x))  \\\\\n",
    "      &= \\frac{\\partial x}{\\partial x}\\sigma(\\beta x)\n",
    "        + x \\frac{\\partial}{\\partial x}\\sigma(\\beta x)  \\\\\n",
    "      &= \\sigma(\\beta x)\n",
    "        + x \\frac{\\partial}{\\partial x}\\sigma(x)\\frac{\\partial}{\\partial x} \\beta x  \\\\\n",
    "      &= \\sigma(\\beta x) + \\beta x (1 - \\sigma(x))  \\\\\n",
    "  \\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "  \\begin{aligned}\n",
    "    \\frac{\\partial}{\\partial \\beta}\\mathit{swish}(x, \\beta)\n",
    "      &= \\frac{\\partial}{\\partial \\beta}(x \\cdot \\sigma(\\beta x)) \\\\\n",
    "      &= \\frac{\\partial x}{\\partial \\beta} \\sigma(\\beta x)\n",
    "        + x\\frac{\\partial}{\\partial \\beta}\\sigma(\\beta x)  \\\\\n",
    "      &= \\frac{\\partial x}{\\partial \\beta} \\sigma(\\beta x)\n",
    "        + x \\frac{\\partial}{\\partial (\\beta x)}\\sigma(\\beta x)\n",
    "          \\frac{\\partial}{\\partial \\beta} \\beta x \\\\\n",
    "      &= x^2 \\sigma(\\beta x)(1 - \\sigma(\\beta x))\n",
    "  \\end{aligned}\n",
    "$$\n",
    "\n",
    "### CELU\n",
    "\n",
    "$$\n",
    "  \\begin{aligned}\n",
    "    \\frac{\\partial}{\\partial x} \\mathit{CELU}(x, \\alpha) \n",
    "      &= \\begin{cases}\n",
    "        \\frac{\\partial}{\\partial x} x & \\text{if } x \\geq 0 \\\\\n",
    "        \\frac{\\partial}{\\partial x} \\alpha (e^{\\frac{x}{\\alpha}} - 1) & \\text{otherwise}\n",
    "      \\end{cases} \\\\\n",
    "      &= \\begin{cases}\n",
    "        1 & \\text{if } x \\geq 0 \\\\\n",
    "        e^{\\frac{x}{\\alpha}} & \\text{otherwise}\n",
    "      \\end{cases}\n",
    "  \\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "  \\frac{\\partial}{\\partial \\alpha} \\mathit{CELU}(x, \\alpha) \n",
    "    &= \\begin{cases}\n",
    "      \\frac{\\partial}{\\partial \\alpha} x & \\text{if } x \\geq 0 \\\\\n",
    "      \\frac{\\partial}{\\partial \\alpha} \\alpha (e^{\\frac{x}{\\alpha}} - 1) & \\text{otherwise}\n",
    "    \\end{cases} \\\\\n",
    "    &= \\begin{cases}\n",
    "      0 & \\text{if } x \\geq 0 \\\\\n",
    "      -\\frac{x e^{\\frac{x}{\\alpha}}}{\\alpha} + e^{\\frac{x}{\\alpha}} - 1 & \\text{otherwise}\n",
    "    \\end{cases}\n",
    "\\end{aligned}\n",
    "$$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "# region : Activation functions\n",
    "def sig(t: Tensor) -> Tensor:\n",
    "    return torch.reciprocal(1 + torch.exp(-1 * t))\n",
    "\n",
    "\n",
    "def tanh(t: Tensor) -> Tensor:\n",
    "    exp_t = torch.exp(t)\n",
    "    exp_neg_t = torch.exp(-1 * t)\n",
    "    return (exp_t - exp_neg_t) * torch.reciprocal(exp_t + exp_neg_t)\n",
    "\n",
    "\n",
    "def relu(t: Tensor) -> Tensor:\n",
    "    \"\"\" Rectifier activation function.\n",
    "        The relu function of a tensor T is the element-wise max between 0 and the appropriate \n",
    "        element of T.\n",
    "    \"\"\"\n",
    "    tensor = t if torch.is_tensor(t) else torch.tensor(t)\n",
    "    return torch.max(tensor, torch.zeros_like(tensor))\n",
    "\n",
    "\n",
    "def swish(t: Tensor, beta: float) -> Tensor:\n",
    "    \"\"\" Swish activation function proposed by Ramachandran et al. on their paper \"Searching for\n",
    "        Activation Functions\" (arXiv:1710.05941v2).\n",
    "        The Swish function of a tensor T is defined as: T * sigmoid(beta * T).\n",
    "    \"\"\"\n",
    "    tensor = t if torch.is_tensor(t) else torch.tensor(t)\n",
    "    beta_tensor = torch.full_like(tensor, beta)\n",
    "    return tensor * sig(beta_tensor * tensor)\n",
    "\n",
    "\n",
    "def celu(t: Tensor, alpha: float) -> Tensor:\n",
    "    \"\"\" Continuously Differentiable Exponential Linear Units function as proposed by Barron on his\n",
    "        paper \"Continuously Differentiable Exponential Linear Units\" (arXiv:1704.07483).\n",
    "        The CELU function of a tensor T is:\n",
    "            - T[i] when T[i] >= 0\n",
    "            - alpha * (exp(T[i] / alpha) - 1)\n",
    "        for each element i of the tensor T.\n",
    "    \"\"\"\n",
    "    tensor = t if torch.is_tensor(t) else torch.tensor(t)\n",
    "    zero_tensor = torch.zeros_like(tensor)\n",
    "    alpha_tensor = torch.full_like(tensor, alpha)\n",
    "    return torch.max(zero_tensor, tensor) + torch.min(zero_tensor, alpha_tensor * (\n",
    "            torch.exp(tensor / alpha_tensor) - torch.full_like(tensor, 1)))\n",
    "\n",
    "\n",
    "def softmax(t: Tensor, dim: int, stable=True) -> Tensor:\n",
    "    \"\"\" Softmax function.\n",
    "        The function stabilizes the values of a sequence of real values by generating a new\n",
    "        sequence: S = exp(X) / sum(exp(*X)).\n",
    "    \"\"\"\n",
    "    if stable:\n",
    "        t -= t.max(dim=dim, keepdim=True)[0]\n",
    "    exp = torch.exp(t)\n",
    "    return exp / torch.sum(exp, dim=dim, keepdim=True)\n",
    "\n",
    "\n",
    "# endregion\n",
    "# region : Derivatives\n",
    "def d_dx_sigmoid(x: Tensor) -> Tensor:\n",
    "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "    return torch.ones_like(x) - sig(x)\n",
    "\n",
    "\n",
    "def d_dx_relu(x: Tensor) -> Tensor:\n",
    "    \"\"\"d/dx ReLU(x)\"\"\"\n",
    "    return torch.ones_like(x) if x > 0 else torch.zeros_like(x)\n",
    "\n",
    "\n",
    "def d_dx_swish(x: Tensor, beta: float) -> Tensor:\n",
    "    \"\"\"d/dx swish(x, beta)\"\"\"\n",
    "    return sig(torch.mul(beta, x))\n",
    "\n",
    "\n",
    "def d_db_swish(x: Tensor, beta: float) -> Tensor:\n",
    "    \"\"\"d/d(beta) swish(x, beta)\"\"\"\n",
    "    sig_bx = sig(torch.mul(beta, x))\n",
    "    return torch.square(x) * sig_bx * (torch.ones_like(sig_bx) - sig_bx)\n",
    "\n",
    "\n",
    "def d_dx_celu(x: Tensor, alpha: float) -> Tensor:\n",
    "    \"\"\"d/dx CELU(x, alpha)\"\"\"\n",
    "    return torch.ones_like(x) if x >= 0 else torch.exp(torch.div(x, alpha))\n",
    "\n",
    "\n",
    "def d_da_celu(x: Tensor, alpha: float) -> Tensor:\n",
    "    \"\"\"d/d(alpha) CELU(x, alpha)\"\"\"\n",
    "    return torch.zeros_like(x) if x >= 0 else torch.mul(-1, torch.div(\n",
    "        torch.mul(x, torch.exp(torch.div(x, alpha))), alpha)) + torch.exp(\n",
    "        torch.div(x, alpha)) - torch.ones_like(x)\n",
    "# endregion"
   ]
  }
 ]
}